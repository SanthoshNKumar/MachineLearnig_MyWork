{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression : Logistic Regression is used when the dependent variable(target) is categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def featureNormalization(X):\n",
    "    mean=np.mean(X,axis=0)\n",
    "    std=np.std(X,axis=0)\n",
    "    \n",
    "    X_norm = (X - mean)/std\n",
    "    \n",
    "    return X_norm , mean , std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\MyWork\\\\MyLearning\\\\ML\\\\Files\\\\DataSet\\\\ex2data1.csv\",header=None)\n",
    "\n",
    "X = df.iloc[:,0:2]\n",
    "y = df.iloc[:,-1]\n",
    "\n",
    "m = X.shape[0]  # rows\n",
    "n = X.shape[1] # columns\n",
    "\n",
    "X = featureNormalization(X)[0]\n",
    "\n",
    "X = np.append(np.ones((m,1)),X,axis=1)\n",
    "y = y[:, np.newaxis]\n",
    "\n",
    "theta = np.zeros((n+1,1))\n",
    "alpha =1\n",
    "J_history =[]\n",
    "epochs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.6931471805599453\n",
      "Cost: 0.5594032425456552\n",
      "Cost: 0.4805876648533871\n",
      "Cost: 0.4304634223593616\n",
      "Cost: 0.3960564700549833\n",
      "Cost: 0.37096692788942365\n",
      "Cost: 0.351811438215547\n",
      "Cost: 0.3366625823479172\n",
      "Cost: 0.32434847924981564\n",
      "Cost: 0.3141169879977177\n",
      "Cost: 0.3054632898746156\n",
      "Cost: 0.29803573064121336\n",
      "Cost: 0.291581583075616\n",
      "Cost: 0.2859143408821499\n",
      "Cost: 0.28089320442020754\n",
      "Cost: 0.27640976797974876\n",
      "Cost: 0.2723791206465764\n",
      "Cost: 0.26873374092404084\n",
      "Cost: 0.26541921086265025\n",
      "Cost: 0.2623911455046634\n",
      "Cost: 0.25961295257859074\n",
      "Cost: 0.2570541709629099\n",
      "Cost: 0.2546892200293839\n",
      "Cost: 0.25249644553195266\n",
      "Cost: 0.2504573827607224\n",
      "Cost: 0.2485561810748081\n",
      "Cost: 0.24677914982076546\n",
      "Cost: 0.24511439661778678\n",
      "Cost: 0.24355153668345658\n",
      "Cost: 0.2420814573412626\n",
      "Cost: 0.24069612578706728\n",
      "Cost: 0.23938843105923618\n",
      "Cost: 0.2381520532694505\n",
      "Cost: 0.23698135472344695\n",
      "Cost: 0.23587128874252009\n",
      "Cost: 0.23481732289271778\n",
      "Cost: 0.23381537401406827\n",
      "Cost: 0.2328617529706419\n",
      "Cost: 0.23195311745280292\n",
      "Cost: 0.231086431484239\n",
      "Cost: 0.2302589305394015\n",
      "Cost: 0.22946809137760052\n",
      "Cost: 0.22871160586000597\n",
      "Cost: 0.22798735814416737\n",
      "Cost: 0.2272934047542032\n",
      "Cost: 0.22662795710875627\n",
      "Cost: 0.22598936615723067\n",
      "Cost: 0.22537610883082476\n",
      "Cost: 0.22478677606094838\n",
      "Cost: 0.22422006215565937\n",
      "Cost: 0.22367475535631928\n",
      "Cost: 0.22314972942295022\n",
      "Cost: 0.22264393611874897\n",
      "Cost: 0.2221563984826461\n",
      "Cost: 0.22168620479432008\n",
      "Cost: 0.22123250314918916\n",
      "Cost: 0.22079449657201353\n",
      "Cost: 0.2203714386071922\n",
      "Cost: 0.2199626293318924\n",
      "Cost: 0.21956741174504235\n",
      "Cost: 0.21918516849112554\n",
      "Cost: 0.21881531888279857\n",
      "Cost: 0.21845731619073352\n",
      "Cost: 0.2181106451728751\n",
      "Cost: 0.21777481981858293\n",
      "Cost: 0.21744938128598013\n",
      "Cost: 0.21713389601330818\n",
      "Cost: 0.21682795398725657\n",
      "Cost: 0.21653116715312212\n",
      "Cost: 0.21624316795331633\n",
      "Cost: 0.21596360798218958\n",
      "Cost: 0.2156921567464251\n",
      "Cost: 0.2154285005213806\n",
      "Cost: 0.21517234129475238\n",
      "Cost: 0.21492339578981792\n",
      "Cost: 0.21468139456129287\n",
      "Cost: 0.21444608115753053\n",
      "Cost: 0.21421721134340804\n",
      "Cost: 0.21399455237879111\n",
      "Cost: 0.21377788234795822\n",
      "Cost: 0.2135669895358024\n",
      "Cost: 0.21336167184701563\n",
      "Cost: 0.21316173626481663\n",
      "Cost: 0.21296699834608987\n",
      "Cost: 0.21277728175009025\n",
      "Cost: 0.21259241779811797\n",
      "Cost: 0.21241224506179845\n",
      "Cost: 0.2122366089778062\n",
      "Cost: 0.21206536148705835\n",
      "Cost: 0.21189836069657036\n",
      "Cost: 0.2117354705623195\n",
      "Cost: 0.21157656059159777\n",
      "Cost: 0.21142150556346206\n",
      "Cost: 0.2112701852660019\n",
      "Cost: 0.21112248424924865\n",
      "Cost: 0.2109782915926423\n",
      "Cost: 0.21083750068605978\n",
      "Cost: 0.2107000090234843\n",
      "Cost: 0.21056571800846707\n",
      "Cost: 0.21043453277059818\n",
      "Cost: 0.21030636199226163\n",
      "Cost: 0.21018111774500448\n",
      "Cost: 0.21005871533490036\n",
      "Cost: 0.2099390731563313\n",
      "Cost: 0.20982211255365601\n",
      "Cost: 0.2097077576902692\n",
      "Cost: 0.20959593542459382\n",
      "Cost: 0.20948657519257796\n",
      "Cost: 0.20937960889630108\n",
      "Cost: 0.20927497079831964\n",
      "Cost: 0.20917259742140754\n",
      "Cost: 0.20907242745337354\n",
      "Cost: 0.2089744016566537\n",
      "Cost: 0.20887846278240244\n",
      "Cost: 0.20878455548882183\n",
      "Cost: 0.2086926262634847\n",
      "Cost: 0.20860262334942636\n",
      "Cost: 0.2085144966747916\n",
      "Cost: 0.20842819778583713\n",
      "Cost: 0.208343679783106\n",
      "Cost: 0.2082608972605968\n",
      "Cost: 0.20817980624776636\n",
      "Cost: 0.2081003641542112\n",
      "Cost: 0.2080225297168848\n",
      "Cost: 0.20794626294971566\n",
      "Cost: 0.20787152509549825\n",
      "Cost: 0.2077982785799393\n",
      "Cost: 0.20772648696774607\n",
      "Cost: 0.20765611492065167\n",
      "Cost: 0.20758712815727784\n",
      "Cost: 0.20751949341474235\n",
      "Cost: 0.2074531784119227\n",
      "Cost: 0.20738815181429296\n",
      "Cost: 0.20732438320025512\n",
      "Cost: 0.20726184302889294\n",
      "Cost: 0.2072005026090761\n",
      "Cost: 0.20714033406985097\n",
      "Cost: 0.20708131033205468\n",
      "Cost: 0.20702340508109476\n",
      "Cost: 0.20696659274083729\n",
      "Cost: 0.20691084844855323\n",
      "Cost: 0.2068561480308712\n",
      "Cost: 0.20680246798069135\n",
      "Cost: 0.20674978543501457\n",
      "Cost: 0.20669807815364571\n",
      "Cost: 0.20664732449873047\n",
      "Cost: 0.2065975034150884\n",
      "Cost: 0.20654859441130594\n",
      "Cost: 0.2065005775415545\n",
      "Cost: 0.2064534333881041\n"
     ]
    }
   ],
   "source": [
    "for i in range(150):\n",
    "    z = np.dot(X,theta)\n",
    "    predictions = 1 /(1+ np.exp(-z))\n",
    "    error_class1 = -y * np.log(predictions)\n",
    "    error_class2 = (1-y) * np.log(1-predictions)\n",
    "    error = error_class1 - error_class2\n",
    "    cost = (1/m) * np.sum(error)\n",
    "    print(\"Cost:\",cost)\n",
    "    grad = (1/m) * np.dot(X.transpose(),(predictions - y))\n",
    "    theta = theta - (alpha * grad)\n",
    "    J_history.append(cost)\n",
    "    epochs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
